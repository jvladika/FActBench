{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7312a29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/anumafzal/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "from typing import List, Optional\n",
    "import sys\n",
    "from FActScore.factscore.factscorer import FactScorer\n",
    "from utils.wandb_utils import wandb_init_run, wandb_push_json, wandb_push_table\n",
    "from utils.fscore_utils import csv_to_jsonl_for_factscore, regenerate_text, flatten_hallucinations\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "nltk.download('punkt_tab')\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from argparse import Namespace\n",
    "import os\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "fs_logs_available = {}\n",
    "\n",
    "class GenFact:\n",
    "    def __init__(self, args: dict = None):\n",
    "        print (args)\n",
    "\n",
    "        #self.args = parse_options(sys.argv[1:] if args is None else args)\n",
    "        \n",
    "        args['gamma'] = 10\n",
    "        args['data_dir']  =\"./FActScore/.cache/factscore/\"\n",
    "        args['model_dir']  = \"./FActScore/.cache/factscore/\"\n",
    "        args['cache_dir'] =\"./FActScore/.cache/factscore/\"\n",
    "        args['knowledge_source'] = None\n",
    "          \n",
    "\n",
    "        args['cost_estimate'] = \"consider_cache\"\n",
    "        args['abstain_detection_type'] = None,\n",
    "        args['use_atomic_facts']= False\n",
    "\n",
    "        args['verbose'] = False\n",
    "\n",
    "        #parser['print_rate_limit_error'] =\"store_true\"\n",
    "        args['n_samples'] = None\n",
    "        \n",
    "        \n",
    "        args = Namespace(**args)\n",
    "\n",
    "        self.args = args\n",
    "        print (self.args)\n",
    "        logging.basicConfig(format='%(asctime)s - %(name)s - %(message)s',\n",
    "                            datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                            level=logging.ERROR )\n",
    "\n",
    "        self.log_dir = self.create_log_folder()\n",
    "        self.fs = FactScorer(model_name=self.args.model_name,\n",
    "                        data_dir=self.args.data_dir,\n",
    "                        model_dir=self.args.model_dir,\n",
    "                        cache_dir=self.args.cache_dir,\n",
    "                        openai_key=self.args.openai_key,\n",
    "                        cost_estimate=self.args.cost_estimate,\n",
    "                        abstain_detection_type=self.args.abstain_detection_type,\n",
    "                        grounding_provided=self.args.grounding_provided)\n",
    "\n",
    "    def run_factscrorer(self, grounding_provided:bool) -> dict:\n",
    "\n",
    "        tot = 0\n",
    "        topics, generations, atomic_facts, groundings = [], [], [], []\n",
    "        with open(self.args.input_path) as f:\n",
    "            for line in f:\n",
    "                dp = json.loads(line)\n",
    "                tot += 1\n",
    "\n",
    "                if self.args.use_atomic_facts:\n",
    "                    assert \"annotations\" in dp, \"You can specify `--use_atomic_facts` only when atomic facts are available in the input data already.\"\n",
    "                    if dp[\"annotations\"] is None:\n",
    "                        continue\n",
    "                    topics.append(dp[\"topic\"])\n",
    "                    generations.append(dp[\"output\"])\n",
    "                    atomic_facts.append(\n",
    "                        [atom[\"text\"] for sent in dp[\"annotations\"] for atom in sent[\"model-atomic-facts\"]])\n",
    "                else:\n",
    "                    topics.append(dp[\"topic\"])\n",
    "                    generations.append(dp[\"output\"])\n",
    "                if self.args.grounding_provided:\n",
    "                    groundings.append(dp[\"input\"])\n",
    "\n",
    "                if self.args.n_samples is not None and tot == args.n_samples:\n",
    "                    break\n",
    "        out = self.fs.get_score(topics=topics,\n",
    "                           generations=generations,\n",
    "                           groundings=groundings,\n",
    "                           gamma=self.args.gamma,\n",
    "                           atomic_facts=atomic_facts if self.args.use_atomic_facts else None,\n",
    "                           knowledge_source=self.args.knowledge_source,\n",
    "                           verbose=self.args.verbose,\n",
    "                           grounding_provided=grounding_provided)\n",
    "        print (\"Using intrinsic Fact Checking\")\n",
    "        logging.critical(\"FActScore = %.1f%%\" % (100 * out[\"score\"]))\n",
    "        if \"init_score\" in out:\n",
    "            logging.critical(\"FActScore w/o length penalty = %.1f%%\" % (100 * out[\"init_score\"]))\n",
    "        logging.critical(\"Respond ratio = %.1f%%\" % (100 * out[\"respond_ratio\"]))\n",
    "        logging.critical(\"# Atomic facts per valid response = %.1f\" % (out[\"num_facts_per_response\"]))\n",
    "\n",
    "        # Save out as a json file\n",
    "        with open(args.input_path.replace(\".jsonl\", f\"_factscore_output.json\"), 'w') as f:\n",
    "            f.write(json.dumps(out) + \"\\n\")\n",
    "\n",
    "        self.factscore_logs = {\"score\": out[\"score\"],\"topics\": topics, \"decisions\": out[\"decisions\"], \"wrong_facts\": out[\"wrong_facts\"], \"groundings\": groundings,\n",
    "                               \"generations\": generations, \"grounding_provided\": grounding_provided}\n",
    "\n",
    "        return self.factscore_logs\n",
    "\n",
    "    def write_logs(self, out:json, fname:str):\n",
    "        fname = os.path.join(self.log_dir,fname)\n",
    "        with open(fname, 'w') as fp:\n",
    "            json.dump(out, fp)\n",
    "\n",
    "    def fs_get_extrinsic_af(self, topics, wrong_facts, groundings,generations, grounding_provided):\n",
    "        # Check if the wrongly classified facts are \"wrong\" or just not present in the article.\n",
    "        extrinsic_af = self.fs.get_extrinsic_af(topics=topics, wrong_facts=wrong_facts, groundings=groundings,\n",
    "                                                  generations=generations, grounding_provided=grounding_provided)\n",
    "\n",
    "\n",
    "        return extrinsic_af\n",
    "    def fs_extrinsic_score(self,fs_extrinsic_af:dict):\n",
    "        extrinsic_out = self.fs.get_extrinsic_score(topics = self.factscore_logs[\"topics\"], extrinsic_facts=fs_extrinsic_af[\"extrinsic_facts\"],\n",
    "                                                    generations = self.factscore_logs[\"generations\"],  verbose=False,\n",
    "                                                    grounding_provided=False)\n",
    "        return extrinsic_out\n",
    "\n",
    "    def create_log_folder(self, ):\n",
    "        date_time =  '{date:%Y-%m-%d_%H-%M-%S}'.format( date=datetime.now() )\n",
    "        run_name = os.path.basename(self.args.input_path).replace('.jsonl','')\n",
    "\n",
    "        folder = os.path.join(\"results\",\"genfact\", run_name, date_time)\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        print(f\"Run outputs would be locally stored at {folder}\")\n",
    "        return folder\n",
    "\n",
    "    def get_updated_score(self, factscore_out, fs_extrinsic_af) ->float:\n",
    "        decision_before = factscore_out[\"decisions\"]\n",
    "        decision_after = fs_extrinsic_af[\"decisions\"]\n",
    "        count = 0\n",
    "\n",
    "\n",
    "        for idx, afs in enumerate(decision_after):\n",
    "            if len(afs) > 0:\n",
    "                for af in afs:\n",
    "                    if decision_before[idx][af[\"idx\"]]['is_supported'] != af[\"is_supported\"]:\n",
    "                        print(f\"Updating the decision for the Atomic Fact: {af} for sample {idx}\")\n",
    "                        decision_before[idx][af[\"idx\"]]['is_supported'] = af[\"is_supported\"]\n",
    "                        count += 1\n",
    "        scores = [np.mean([d[\"is_supported\"]  for d in decisions]) for decisions in decision_before]\n",
    "        hallucinations = [[d for d in decisions if not d[\"is_supported\"]] for decisions in decision_before]\n",
    "\n",
    "        updated_score = np.mean(scores)\n",
    "        logging.critical(\"FActScore After extrinsic check = %.1f%%\" % (100 * updated_score))\n",
    "        logging.critical(f\"Updated decision on {str(count)} Facts after running Extrinsic check\")\n",
    "        #if \"init_score\" in extrinsic_out:\n",
    "        #    logging.critical(\"FActScore w/o length penalty = %.1f%%\" % (100 * out[\"init_score\"]))\n",
    "        #logging.critical(\"Respond ratio = %.1f%%\" % (100 * out[\"respond_ratio\"]))\n",
    "        #logging.critical(\"# Atomic facts per valid response = %.1f\" % (out[\"num_facts_per_response\"]))\n",
    "        return updated_score, hallucinations\n",
    "\n",
    "\n",
    "class DebertaNli:\n",
    "    def __init__(self, score_out, decisions, groundings, fs):\n",
    "\n",
    "        self.score_out = score_out\n",
    "        self.decisions = decisions\n",
    "        self.groundings = groundings\n",
    "        self.fs = fs\n",
    "\n",
    "        self.model_name = \"tasksource/deberta-base-long-nli\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name)\n",
    "        self.model.to(device)\n",
    "\n",
    "    def get_nli_class(self, premise, hypothesis):\n",
    "        model_input = self.tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "        model_output = self.model(model_input[\"input_ids\"].to(device)) \n",
    "        prediction_probs = torch.softmax(model_output[\"logits\"][0], -1).tolist()\n",
    "        prediction_probs = np.array(prediction_probs)\n",
    "\n",
    "        max_index = np.argmax(prediction_probs)\n",
    "        if  max_index == 0:\n",
    "            nli_class = \"entailment\"\n",
    "        elif max_index == 1:\n",
    "            nli_class = \"neutral\"\n",
    "        elif max_index == 2:\n",
    "            nli_class = \"contradiction\"\n",
    "\n",
    "        return nli_class\n",
    "\n",
    "\n",
    "    def check_intrinsic(self,) -> dict:\n",
    "        nli_decisions = list()\n",
    "        deberta_scores = list()\n",
    "        \n",
    "        for data_instance, article in zip(self.decisions, self.groundings):\n",
    "            nli_results = list()\n",
    "\n",
    "            for atom_instance in data_instance:\n",
    "                atom_fact = atom_instance[\"atom\"]\n",
    "                premise = atom_fact\n",
    "                hypothesis = article\n",
    "\n",
    "                nli_class = self.get_nli_class(premise, hypothesis)\n",
    "                nli_results.append(nli_class)\n",
    "\n",
    "            nli_decisions.append(nli_results)\n",
    "        \n",
    "        new_decisions = list()\n",
    "        wrong_facts = list()\n",
    "\n",
    "        for decision, nli_prediction in zip(self.decisions, nli_decisions):\n",
    "            new_list = list()\n",
    "            for dec, pred in zip(decision, nli_prediction):\n",
    "                dec[\"nli_intrinsic\"] = pred\n",
    "                dec[\"nli_supported_intrinsic\"] = True if pred == \"entailment\" else False\n",
    "                new_list.append(dec)\n",
    "\n",
    "\n",
    "            debscore = np.mean([d[\"nli_supported_intrinsic\"] for d in decision])\n",
    "            deberta_scores.append(debscore)\n",
    "            new_decisions.append(new_list)\n",
    "\n",
    "            wfs = [{\"atom\":d[\"atom\"], \"idx\":idx}  for idx, d in enumerate(new_list) if not d[\"nli_supported_intrinsic\"]]\n",
    "            wrong_facts.append(wfs)\n",
    "\n",
    "            for d in new_list:\n",
    "                if not d[\"nli_supported_intrinsic\"]:\n",
    "                    passages = self.fs.search_passage_till_success(topic = '', atom = d[\"atom\"], generation=d[\"atom\"],\n",
    "                                                               knowledge_source=\"enwiki-20230401\")\n",
    "                    context = \"\"\n",
    "                    for psg_idx, psg in enumerate(reversed(passages)):\n",
    "                        context += \"Title: {}\\nText: {}\\n\\n\".format(psg[\"title\"], psg[\"text\"].replace(\"<s>\", \"\").replace(\"</s>\", \"\"))\n",
    "                    context = context.strip()\n",
    "\n",
    "                    d[\"wiki_context\"] = context\n",
    "\n",
    "\n",
    "        self.decisions = new_decisions            \n",
    "        self.score_out[\"decisions\"] = new_decisions\n",
    "\n",
    "        self.score_out[\"deberta_score_intrinsic\"] = np.mean(deberta_scores)\n",
    "        self.score_out[\"deberta_wrong_facts\"] = wrong_facts\n",
    "\n",
    "        logging.critical(\"Deberta Score (intrinsic) = %.1f%%\" % (100 * np.mean(deberta_scores)))\n",
    "\n",
    "        return self.score_out, np.mean(deberta_scores)\n",
    "    \n",
    "\n",
    "    def check_extrinsic(self, wrong_facts) -> dict:\n",
    "        nli_decisions = list()\n",
    "        deberta_scores = list()\n",
    "\n",
    "        for data_instance, wf in zip(self.decisions, wrong_facts):\n",
    "            nli_results = list()\n",
    "\n",
    "            fact_idx = 0\n",
    "            wf_indices = [w[\"idx\"] for w in wf]\n",
    "\n",
    "            for atom_instance in data_instance:\n",
    "                if fact_idx not in wf_indices:\n",
    "                    nli_results.append((\"none\", fact_idx))\n",
    "                    fact_idx += 1\n",
    "                    continue\n",
    "\n",
    "                atom_fact = atom_instance[\"atom\"]\n",
    "                atom_wiki_context = atom_instance[\"wiki_context\"]\n",
    "\n",
    "                premise = atom_fact\n",
    "                hypothesis = atom_wiki_context\n",
    "\n",
    "                model_input = self.tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "                model_output = self.model(model_input[\"input_ids\"].to(device)) \n",
    "                prediction_probs = torch.softmax(model_output[\"logits\"][0], -1).tolist()\n",
    "                prediction_probs = np.array(prediction_probs)\n",
    "\n",
    "                max_index = np.argmax(prediction_probs)\n",
    "                if  max_index == 0:\n",
    "                    nli_class = \"entailment\"\n",
    "                elif max_index == 1:\n",
    "                    nli_class = \"neutral\"\n",
    "                elif max_index == 2:\n",
    "                    nli_class = \"contradiction\"\n",
    "                nli_results.append((nli_class, fact_idx))\n",
    "\n",
    "                fact_idx += 1\n",
    "\n",
    "            nli_decisions.append(nli_results)\n",
    "\n",
    "        new_decisions = list()\n",
    "        for decision, nli_prediction in zip(self.decisions, nli_decisions):\n",
    "            new_list = list()\n",
    "\n",
    "            atom_dec_idx = 0\n",
    "            wrong_fact_indices = [n[1] for n in nli_prediction if n[0] != \"none\"]\n",
    "            for dec, pred in zip(decision, nli_prediction):\n",
    "\n",
    "                if atom_dec_idx in wrong_fact_indices:\n",
    "                    dec[\"nli_extrinsic\"] = pred[0]\n",
    "                    dec[\"nli_supported_extrinsic\"] = True if pred[0] == \"entailment\" else False\n",
    "                    dec[\"nli_final_supported\"] = dec[\"nli_supported_extrinsic\"]\n",
    "                    new_list.append(dec)\n",
    "                else:\n",
    "                    dec[\"nli_final_supported\"] = dec[\"nli_supported_intrinsic\"]\n",
    "                    new_list.append(dec)\n",
    "\n",
    "                atom_dec_idx += 1\n",
    "\n",
    "            #debscore = np.mean([d[\"nli_supported_extrinsic\"] for d in decision])\n",
    "            #deberta_scores.append(debscore)\n",
    "            debscore = np.mean([d[\"nli_final_supported\"] for d in new_list])\n",
    "            deberta_scores.append(debscore)\n",
    "\n",
    "            new_decisions.append(new_list)\n",
    "\n",
    "        self.decisions = new_decisions            \n",
    "        self.score_out[\"decisions\"] = new_decisions\n",
    "        self.score_out[\"deberta_score_final\"] = np.mean(deberta_scores)\n",
    "        logging.critical(\"Deberta score (final, after extrinsic) = %.1f%%\" % (100 * np.mean(deberta_scores)))        \n",
    "\n",
    "        return self.score_out, np.mean(deberta_scores)\n",
    "\n",
    "def get_pooled_score(deberta_extrinsic_out):\n",
    "    decisions = deberta_extrinsic_out[\"decisions\"]\n",
    "    new_decisions = list()\n",
    "    pooled_scores = list()\n",
    "\n",
    "    for instance_decisions in decisions:\n",
    "        new_list = list()\n",
    "    \n",
    "        for dec in instance_decisions:\n",
    "            deberta_final = dec[\"nli_final_supported\"]\n",
    "            factscore_final = dec[\"is_supported\"]\n",
    "\n",
    "            #Pooled decision is True if both FactScore and NLI predictions are True.\n",
    "            pooled_final = deberta_final and factscore_final\n",
    "            dec[\"pooled_supported\"] = pooled_final\n",
    "            new_list.append(dec)\n",
    "\n",
    "        poolscore = np.mean([d[\"pooled_supported\"] for d in new_list])\n",
    "        pooled_scores.append(poolscore)\n",
    "\n",
    "        new_decisions.append(new_list)\n",
    "\n",
    "    logging.critical(\"Pooled score (final) = %.1f%%\" % (100 * np.mean(pooled_scores)))        \n",
    "\n",
    "    return new_decisions, np.mean(pooled_scores)\n",
    "\n",
    "\n",
    "def parse_options(parser: dict)-> dict:\n",
    "\n",
    "    parser['gamma'] = 10\n",
    "    parser['data_dir']  =\"./FActScore/.cache/factscore/\"\n",
    "    parser['model_dir']  = \"./FActScore/.cache/factscore/\"\n",
    "    parser['cache_dir'] =\"./FActScore/.cache/factscore/\"\n",
    "    parser['knowledge_source'] = None\n",
    "          \n",
    "\n",
    "    parser['cost_estimate'] = \"consider_cache\"\n",
    "    parser['abstain_detection_type'] = None,\n",
    "    parser['use_atomic_facts']= \"store_true\"\n",
    "                        \n",
    "    parser['verbose'] = False\n",
    "                            \n",
    "    #parser['print_rate_limit_error'] =\"store_true\"\n",
    "    parser['n_samples'] = None\n",
    "    \n",
    "    return parser\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb8b755c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def main(input_path = \"/Users/anumafzal/PycharmProjects/FactSumm/results/1od31evk_MODEL_meta-llama-Meta-Llama-3.1-70B-Instruct-Turbo_DS_pubmed_TASK_summarization.jsonl\",\n",
    "        model_key = \"GPT4-mini\",\n",
    "        openai_key = 'api.key',\n",
    "        grounding_provided = 'True'):\n",
    "    \n",
    "    args = dict()\n",
    "    args['input_path'] = input_path\n",
    "    args['model_name'] = model_key\n",
    "    args['openai_key'] = openai_key\n",
    "    args['grounding_provided'] = grounding_provided\n",
    "\n",
    "\n",
    "    csv_results_dir = \"results/\"\n",
    "    #jsonl_paths = csv_to_jsonl_for_factscore(csv_results_dir)\n",
    "\n",
    "\n",
    "    genFact = GenFact(args)\n",
    "\n",
    "    wandb_init_run(run_path=args['input_path'], config = genFact.args)\n",
    "\n",
    "\n",
    "    print (\"Running Vanilla FactScore\")\n",
    "    factscore_out_vanilla = genFact.run_factscrorer(grounding_provided = False )\n",
    "    genFact.write_logs(factscore_out_vanilla, fname=\"factscore_vanilla.json\")\n",
    "\n",
    "    print (\"Running Factscore with grounded document\")\n",
    "    factscore_out = genFact.run_factscrorer(grounding_provided=args.grounding_provided)\n",
    "    genFact.write_logs(factscore_out, fname=\"factscore_grounded.json\")\n",
    "\n",
    "\n",
    "    fs_extrinsic_af = genFact.fs_get_extrinsic_af(topics = factscore_out[\"topics\"], wrong_facts = factscore_out[\"wrong_facts\"],\n",
    "                    groundings=  factscore_out[\"groundings\"], generations = factscore_out[\"generations\"],\n",
    "                                                grounding_provided= factscore_out[\"grounding_provided\"])\n",
    "    fs_extrinsic_out = genFact.fs_extrinsic_score(fs_extrinsic_af)\n",
    "    genFact.write_logs(factscore_out, fname=\"factscore_grounded_extrinsic.json\")\n",
    "\n",
    "\n",
    "    fs_updated_score, fs_updated_wrong_facts = genFact.get_updated_score(factscore_out,fs_extrinsic_out)\n",
    "    wandb_table = {\"fs_wiki\": factscore_out_vanilla[\"score\"], \"fs_grounded\": factscore_out[\"score\"],\n",
    "                       \"fs_grounded_wiki\": fs_updated_score}\n",
    "    wandb_push_json(wandb_table)\n",
    "\n",
    "        # test regeneration\n",
    "    fs_regenerations = regenerate_text(factscore_out[\"generations\"], flatten_hallucinations(fs_updated_wrong_facts))\n",
    "\n",
    "    wandb_table = {\"generations\": factscore_out[\"generations\"], \"hallucinations\": fs_updated_wrong_facts,\n",
    "                       \"regenerations\": fs_regenerations}\n",
    "    wandb_push_table(wandb_table)\n",
    "\n",
    "    #Creates new class for deberta predictions. Loads a model from HuggingFace.\n",
    "    deberta_nli = DebertaNli(score_out = factscore_out,\n",
    "                                 decisions =  factscore_out[\"decisions\"],\n",
    "                                  groundings = factscore_out[\"groundings\"],\n",
    "                                  fs = genFact.fs)\n",
    "\n",
    "    #Output is the same as factscore_out, but with a new attribute in dictionaries with NLI predictions. \n",
    "    #Gives intrinsic NLI score.\n",
    "    deberta_out, deberta_intrinsic_score = deberta_nli.check_intrinsic()\n",
    "    genFact.write_logs(deberta_out, fname=\"deberta_grounded.json\")\n",
    "\n",
    "\n",
    "    #Checks the wrong facts with extrinsic checking over Wikipedia. Gives final NLI score.\n",
    "    deberta_nli.score_out = fs_extrinsic_out    \n",
    "    deberta_extrinsic_out, deberta_final_score = deberta_nli.check_extrinsic(factscore_out[\"wrong_facts\"])\n",
    "    genFact.write_logs(deberta_extrinsic_out, fname=\"deberta_grounded_extrinsic.json\")\n",
    "\n",
    "    #Calculates the final pooled prediction (inside of pooled_decisions) and final pooled score.\n",
    "    pooled_decisions, pooled_score = get_pooled_score(deberta_extrinsic_out)\n",
    "\n",
    "    deberta_score_dict = {\n",
    "                       \"deberta_grounded\": factscore_out[\"deberta_score_intrinsic\"],\n",
    "                    \"deberta_grounded_wiki\": deberta_final_score,\"pooled_score\":pooled_score}\n",
    "    wandb_push_json(deberta_score_dict)\n",
    "\n",
    "    db_regeneration = factscore_out[\"generations\"]\n",
    "    db_regenerations = regenerate_text(factscore_out[\"generations\"], flatten_hallucinations(fs_updated_wrong_facts))\n",
    "\n",
    "    wandb_table = {\"generations\": factscore_out[\"generations\"], \"hallucinations\": fs_updated_wrong_facts,\n",
    "                       'regenerations':db_regenerations}\n",
    "    wandb_push_table(wandb_table)\n",
    "\n",
    "    print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b2c96bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_path': '/Users/anumafzal/PycharmProjects/FactSumm/results/1od31evk_MODEL_meta-llama-Meta-Llama-3.1-70B-Instruct-Turbo_DS_pubmed_TASK_summarization.jsonl', 'model_name': 'GPT4-mini', 'openai_key': 'api.key', 'grounding_provided': 'True'}\n",
      "Namespace(input_path='/Users/anumafzal/PycharmProjects/FactSumm/results/1od31evk_MODEL_meta-llama-Meta-Llama-3.1-70B-Instruct-Turbo_DS_pubmed_TASK_summarization.jsonl', model_name='GPT4-mini', openai_key='api.key', grounding_provided='True', gamma=10, data_dir='./FActScore/.cache/factscore/', model_dir='./FActScore/.cache/factscore/', cache_dir='./FActScore/.cache/factscore/', knowledge_source=None, cost_estimate='consider_cache', abstain_detection_type=(None,), use_atomic_facts=False, verbose=False, n_samples=None)\n",
      "Run outputs would be locally stored at results/genfact/1od31evk_MODEL_meta-llama-Meta-Llama-3.1-70B-Instruct-Turbo_DS_pubmed_TASK_summarization/2024-09-10_14-46-32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:e4s1kw55) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">tiny-atoll_meta-llama-Meta-Llama-3.1-70B-Instruct-Turbo_pubmed_summarization</strong> at: <a href='https://wandb.ai/sebis19/factgen/runs/e4s1kw55' target=\"_blank\">https://wandb.ai/sebis19/factgen/runs/e4s1kw55</a><br/> View project at: <a href='https://wandb.ai/sebis19/factgen' target=\"_blank\">https://wandb.ai/sebis19/factgen</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240910_144513-e4s1kw55/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:e4s1kw55). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/anumafzal/PycharmProjects/FactSumm/wandb/run-20240910_144643-t68awnzu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sebis19/factgen/runs/t68awnzu' target=\"_blank\">isometric-agreeance_meta-llama-Meta-Llama-3.1-70B-Instruct-Turbo_pubmed_summarization</a></strong> to <a href='https://wandb.ai/sebis19/factgen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sebis19/factgen' target=\"_blank\">https://wandb.ai/sebis19/factgen</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sebis19/factgen/runs/t68awnzu' target=\"_blank\">https://wandb.ai/sebis19/factgen/runs/t68awnzu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Vanilla FactScore\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 25\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(input_path, model_key, openai_key, grounding_provided)\u001b[0m\n\u001b[1;32m     21\u001b[0m wandb_init_run(run_path\u001b[38;5;241m=\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_path\u001b[39m\u001b[38;5;124m'\u001b[39m], config \u001b[38;5;241m=\u001b[39m genFact\u001b[38;5;241m.\u001b[39margs)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning Vanilla FactScore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m factscore_out_vanilla \u001b[38;5;241m=\u001b[39m \u001b[43mgenFact\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_factscrorer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrounding_provided\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m genFact\u001b[38;5;241m.\u001b[39mwrite_logs(factscore_out_vanilla, fname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfactscore_vanilla.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning Factscore with grounded document\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 91\u001b[0m, in \u001b[0;36mGenFact.run_factscrorer\u001b[0;34m(self, grounding_provided)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_samples \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tot \u001b[38;5;241m==\u001b[39m args\u001b[38;5;241m.\u001b[39mn_samples:\n\u001b[1;32m     90\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtopics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mgenerations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mgroundings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroundings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m                   \u001b[49m\u001b[43matomic_facts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43matomic_facts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_atomic_facts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mknowledge_source\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mknowledge_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mgrounding_provided\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrounding_provided\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing intrinsic Fact Checking\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    100\u001b[0m logging\u001b[38;5;241m.\u001b[39mcritical(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFActScore = \u001b[39m\u001b[38;5;132;01m%.1f\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n",
      "File \u001b[0;32m~/PycharmProjects/FactSumm/FActScore/factscore/factscorer.py:138\u001b[0m, in \u001b[0;36mFactScorer.get_score\u001b[0;34m(self, topics, generations, groundings, gamma, atomic_facts, knowledge_source, verbose, grounding_provided)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maf_generator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 138\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maf_generator \u001b[38;5;241m=\u001b[39m \u001b[43mAtomicFactGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopenai_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mdemon_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdemos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mgpt3_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGPT4o-mini.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;66;03m# estimate the total cost of atomic fact generation\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     total_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/PycharmProjects/FactSumm/FActScore/factscore/atomic_facts.py:22\u001b[0m, in \u001b[0;36mAtomicFactGenerator.__init__\u001b[0;34m(self, key_path, demon_dir, gpt3_cache_file)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key_path, demon_dir, gpt3_cache_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men_core_web_sm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_bio \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdemon_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(demon_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdemons.json\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_bio \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdemons_complex.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/py3.10/lib/python3.10/site-packages/spacy/__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py3.10/lib/python3.10/site-packages/spacy/util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc58526b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py3.10)",
   "language": "python",
   "name": "py3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
